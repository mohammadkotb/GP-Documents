% Chapter 2
\newenvironment{my_itemize}
{\begin{itemize}
  \setlength{\itemsep}{0cm}
  \setlength{\parskip}{0cm}}
{\end{itemize}}
\newenvironment{my_enumerate}
{\begin{enumerate}
  \setlength{\itemsep}{0cm}
  \setlength{\parskip}{0cm}}
{\end{enumerate}}

\chapter{Email Classification and Related Work} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 

\lhead{Chapter 2. \emph{Email Classification and Related Work}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

\section{Introduction}
In chapter 1, a general description of the problem was presented, the motivation to develop Smart Email, and the scope of work were introduced.

%==============================================================================
\newpage

\section{Machine Learning in Automated Text Categorization}
\subsection{Definition of Text Categorization}
Text categorization is the task of assigning a Boolean value to each pair $\langle$ d$_{j}$,c$_{i}$ $\rangle$ $\in$ D X C,  where D is a domain of documents and C = \{c$_{1}$, . . . , c $_{|C|}$\} is a set of predefined categories. A value of T assigned to $\langle$ d$_{j}$,c$_{i}$ $\rangle$ indicates a decision to file d$_{j}$ under c$_{i}$, while a value of F indicates a decision not to file d$_{j}$ under c$_{i}$. More formally, the task is to approximate the unknown target function $\theta$ : D ×C $\rightarrow$ \{T, F\} (that describes how documents ought to be classified) by means of a function $\phi$ : D × C $\rightarrow$ \{T, F\} called the classifier (aka rule, or hypothesis, or model ) such that $\theta$ and $\phi$ coincide as much as possible.\cite{Sebastiani2002}

\subsection{Single-label vs Multi-label Text Categorization}
Classification is a very important topic within supervised learning field. Although the most popular task for classification usually deals with single-label datasets, where every example
is associated with a single label $\lambda$ from a set of disjoint labels L, the multi-label datasets are emerging and gaining interest due to their increasing application to real problems. Multilabel datasets are used when the examples are associated with a set of labels Y $\subseteq$ L, as occurs with email classification, image annotation, or genomics.

Two main tasks can be defined when learning from multilabel
data: 
\begin{itemize}
\item Multi-label classification (MLC) that returns a subset of labels to be associated with a given example (it can be considered as a bipartition of the label set considering relevant and irrelevant elements);
\item label ranking (LR) that returns an ordering of the labels according to their relation with the example. \cite{Carmona2011}
\end{itemize}

\subsection{Category Pivoted vs Document Pivoted Text Categorization}
There are two different ways of using a text classifier. Given d$_{j}$ $\in$ D, we might want
to find all the c$_{i}$ $\in$ C under which it should be filed (document-pivoted categorization
– DPC); alternatively, given c$_{i}$ $\in$ C, we might want to find all the d$_{j}$ $\in$ D that should be filed under it (category-pivoted categorization – CPC).
DPC is thus suitable when documents become available at different moments in time, e.g. in filtering e-mail. CPC is instead suitable when: 
\begin{itemize}
\item a new category c$_{|C|+1}$ may be added to an existing set C = \{c$_{1}$, . . . , c$_{|C|}$\} after a number of documents have already been classified under C;
\item these documents need to be reconsidered for classification under c$_{|C|+1}$.
\end{itemize}. DPC is used more often than CPC, as the former situation is more common than the latter. \cite{Sebastiani2002}

\section{Applications of Email Categorization}
\subsection{Spam Detection}
Spam remains a serious problem today because it continues to be a very profitable business for spammers. Spam takes on various forms from adult content, selling products/services, pharmaceuticals to stock promotions, job offers, etc.

Applying machine learning techniques for spam email detections plays a major role in protecting end users from spam email messages.\cite{peifeng2007}

\subsection{Email Organization}
Email has been an efficient and popular communication mechanism as the number of
Internet users’ increases. Therefore, email management has become an important and growing problem for individuals and organizations because it is prone to misuse. One of the problems that are most paramount is disordered email message, congested and un-structured emails in mail boxes.

It may be very hard to find archived email message, search for previous emails with specified contents or features when the mails are not well structured and organized.

At this stage new effective method for managing information in email, reducing email overloads is developed by classifying emails based on important words. \cite{taiwo2007}

\section{The Machine Learning Approach for Text Classification}

In the 80s the most popular approach (at least in operational settings) for the
creation of automatic document classiﬁers consisted in manually building, by means
of knowledge engineering (KE) techniques, an expert system capable of taking Text Categorization (TC) decisions. Such an expert system would typically consist of a set of manually deﬁned logical rules.

Since the early 90s, the Machine Learning (ML) approach to TC has gained popularity and has eventually become the dominant one. In ML terminology, the classification problem is an activity of supervised learning, since the learning process is 'supervised' by the knowledge of
the categories and of the training instances that belong to them.

The advantages of the ML approach over the KE approach are evident. The engineering eﬀort goes towards the construction not of a classiﬁer, but of an automatic builder of classiﬁers (the learner ). \cite{Sebastiani2002}

\subsection{Training Set, Test Set, and Validation Set}
The ML approach relies on the availability of an initial corpus = \{d$_{1}$, . . . , d$_{|\Omega|}$\} $\subset$ D of documents preclassified under C = \{c$_{1}$, . . . , c$_{|C|}$\}. That is, the values of the total function $\theta$ : D X C $\rightarrow$ \{T, F\} are known for every pair $\langle$ d$_{j}$ , c$_{i}$ $\rangle$ $\in$   $\Omega$ X C. A document d$_{j}$ is a positive example of c$_{i}$ if (d$_{j}$ , c$_{i}$) = T , a negative example of c$_{i}$ if
(d$_{j}$ , c$_{i}$) = F.

The following subsections represents the different data sets used  in different research papers for email classification:

\subsubsection{Datasets used in Email Classification}
\subparagraph{Enron Dataset}
    \begin{my_itemize}
        \item Automatic Categorization of Email into Folders \cite{RON04}
        \item An Object Oriented Email Clustering Model Using  Weighted Similarities 
  between Email Attributes \cite{NARESH10}
        \item Using GNUsmail to Compare Data Stream Mining Methods \cite{JOSE11}
    \end{my_itemize}
\subparagraph{SRI Dataset}
    \begin{my_itemize}
        \item Automatic Categorization of Email into Folders \cite{RON04}
    \end{my_itemize}
\subparagraph{Pine Dataset}
    \begin{my_itemize}
        \item Email classification for contact centers \cite{ANI03}
    \end{my_itemize}
\subparagraph{Private Dataset}
    \begin{my_itemize}
        \item Enterprise Email Classification Based on Social Network Features \cite{MIN11}
        \item E-Classifier: A Bi-Lingual Email Classification System \cite{NOUF08} 
        \item Automatically tagging email by leveraging other users folders \cite{YEHUDA11}
    \end{my_itemize}
\subparagraph{Public Pua}
    \begin{my_itemize}
        \item Email Categorization Using Multi-Stage Classification Technique \cite{MD07}
    \end{my_itemize}

\section{Construction of Text Classifier}
\section{Classifier Evaluation}


\section{Email Classification Taxonomy}
The following table classifies some recent research papers in the field of email classification.

\subsection{Classification according to the different learning algorithms used in different papers}

\begin{center}
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\multicolumn{6}{|c|}{Learning Algorithm} \\
\hline
SVM & Na\"{\i}ve Bayes & Neural Networks & Max. Entropy / Winnow & Nnge / Hoeffing Trees & Graph Mining \\ \hline
Email Classification with Co-training \cite{SVETLANA01} &
Email Classification with Co-training \cite{SVETLANA01} &
Email Classification: Solution with Back Propagation Technique \cite{mous05} & 
Automatic Categorization of Emails into Folders \cite{RON04} &
Using GNUsmail to compare Data Stream Mining Methods for On-line Email Classification \cite{JOSE11} &
A graph Based Approach for Multi-Folder Email Classification \cite{sift02} \\ \hline

Automatic Categorization of Emails into Folders \cite{RON04} &
Automatic Categorization of Emails into Folders \cite{RON04} &
Email Classification Using Semantic Feature Space \cite{YUN08} & 
&
& \\ \hline
\end{tabular}
\end{center}
\newpage

\subsection{Classification according to the different learning capabilities}

\begin{center}
\begin{tabular}{|p{6cm}|p{6cm}|}
\hline
\multicolumn{2}{|c|}{Learning Capability} \\
\hline
On-line Learning & Off-Line Learning 
\\ \hline
Using GNUsmail to Compare Data Stream Mining Methods \cite{JOSE11} &
An Object Oriented Email Clustering Model Using  Weighted Similarities 
between Email Attributes \cite{NARESH10}
\\ \hline

GNUsmail: Open Framework for On-line Email Classification \cite{MANUEL11}
& Content Based Email Classification System by applying Conceptual Maps \cite{BASKARAN09}
\\ \hline

& E-Classifier: A Bi-Lingual Email Classification System \cite{NOUF08}
\\ \hline

& Email classification for contact centers \cite{ANI03}
\\ \hline

& 
Automatic Categorization of Email into Folders \cite{RON04}

\\ \hline

\end{tabular}
\end{center}

\newpage


%================================= FEATURES
\subsection{Different features used for email classification}
This section summarizes the different features used for email classification in different research papers
    \subparagraph{Automatic Categorization of Email into Folders \cite{RON04}}
	\begin{my_itemize}
		\item bag-of-words document representation: messages are represented as vectors of word counts.
		\item Words are downcased.
		\item 100 most frequent words and words that appear only once in the training set are removed, and the remaining words are counted in each message to compose a vector.
		\item In future work, richer representations will be considered, including the following:
			\begin{itemize}
				\item Different sections of each email can be treated differently. For example, the system could create distinct features for words appearing in the header, body, signature, attachments, etc.
				\item Named entities may be highly relevant features.
			\end{itemize}
	\end{my_itemize}

    \subparagraph{Email Classifications For Contact Centers \cite{ANI03}}
		\begin{my_itemize}
			\item Feature sets used for experiments included:
				\begin{itemize}
					\item Non-inflected words.
					\item Noun phrases.
					\item Verb phrases.
					\item Punctuation.
					\item Length of the Email.
					\item Dictionaries.
				\end{itemize}
		\end{my_itemize}

	\subparagraph{Using GNUsmail to Compare Data Stream Mining Methods for On-line Email \cite{JOSE11}}
		\begin{my_itemize}
			\item The main feature of the text preprocessing module is a multi-layer filter structure, responsible for performing feature extraction tasks.
			\item The Inbox and Sent folders are skipped in the learning process because they can be thought of as non-specific folders.
			\item Every mail belonging to any other folder (that is, to any topical folder ) goes through a pipeline of linguistic operators which extract relevant features from it.
			\item As the number of possible features is prohibitively large, only the most relevant ones are selected.
		\end{my_itemize}
   
	\subparagraph{Content Based Email Classification System by applying Conceptual Maps \cite{BASKARAN09}}
		\begin{my_itemize}
			\item Unstructured text: consists of fields like the subject and body.
			\item Categorical text: includes fields such as "to" and "from".
			\item Numeric data: includes such features as the message size, number of
recipients and counts of particular characters.
		\end{my_itemize}

\newpage
\subsection{Chronological sort of classification papers}
\subparagraph{2011}
\begin{my_itemize}
  \item Using GNUsmail to Compare Data Stream Mining Methods for On-line Email Classification \cite{JOSE11}
  \item Enterprise Email Classification Based on Social Network Features \cite{MIN11}
  \item Automatically tagging email by leveraging other users folders \cite{YEHUDA11}
\end{my_itemize}

\subparagraph{2010}
\begin{my_itemize}
  \item An Object Oriented Email Clustering Model Using Weighted Similarities between Emails Attributes \cite{NARESH10}
  \item A Graph-Based Approach for Multi-Folder Email Classification \cite{sift02}
\end{my_itemize}

\subparagraph{2009}
\begin{my_itemize}
  \item Content Based Email Classification System by applying Conceptual Maps \cite{BASKARAN09}
  \item Email Classification: Solution with Back Propagation Technique \cite{mous05}
\end{my_itemize}

\subparagraph{2008}
\begin{my_itemize}
  \item A new approach to Email classification using Concept Vector Space Model \cite{CHAO08}
  \item E-Classifier: A Bi-Lingual Email Classification System \cite{NOUF08}
  \item Ontology based classification and categorization of email \cite{BALAKUMAR08}
  \item Applying Machine learning Algorithms for Email Management \cite{mous03}
\end{my_itemize}

\subparagraph{2007}
\begin{my_itemize}
  \item Email Categorization Using Multi-Stage Classification Technique \cite{MD07}
\end{my_itemize}

\subparagraph{2005}
\begin{my_itemize}
  \item An Email Classification Model Based on Rough Set Theory \cite{WENQING05}
  \item eMailSift: Email Classification Based on Structure and Content \cite{sift01}
\end{my_itemize}

\subparagraph{2004}
\begin{my_itemize}
  \item Automatic Categorization of Email into Folders \cite{RON04}
  \item Co-training with a Single Natural Feature Set Applied to Email Classification \cite{mous04}
\end{my_itemize}

\subparagraph{2003}
\begin{my_itemize}
  \item Email Classifications For Contact Centers \cite{ANI03}
\end{my_itemize}


\subsection{Conclusion}
\begin{my_itemize}
    \item SVM Achieved the best results in most papers, but it is computationally expensive and has the hardest implementation.
    \item The aasic form of Na\"{\i}ve Bayes algorithm has the simplest implementation but has very low classification accuracy compared to other algorithms.
    \item Online learning techniques are still under development, they are very hard to implement but characterized by their
    ability to classify new coming email without rebuilding the model.
    \item Offline learning techniques are used in most papers.
    \item Enron dataset is the most commonly used.
\end{my_itemize}

\newpage
%==============================================================================
\section{Email Summarization Taxonomy}

\subsection{Summarization taxonomy according to different techniques}

\begin{center}
\begin{tabular}{|p{6cm}|p{6cm}|}
\hline
\multicolumn{2}{|c|}{Summarization Techniques} \\
\hline
Extractive Summarization & Question-Answer Pairs detection
\\ \hline
Detection of question-answer pairs in email conversations \cite{LOKESH04} &
Using Question-Answer Pairs in Extractive Summarization of Email Conversations \cite{KATHLEEN07} 
\\ \hline

Summarizing email conversations with clue words \cite{GIUSEPPE07} &
Using Question-Answer Pairs in Extractive Summarization of Email Conversations \cite{KATHLEEN07}
\\ \hline

\end{tabular}
\end{center}


%==============================================================================
\subsection{Chronological sort of summarization papers}

\subparagraph{2001}
\begin{itemize}
  \item Combining Linguistic and Machine Learning Techniques for Email
Summarization \cite{SMAR01}
\end{itemize}

\subparagraph{2004}
\begin{itemize}
  \item Detection of question-answer pairs in email conversations \cite{LOKESH04}
\end{itemize}

\subparagraph{2007}
\begin{itemize}
  \item Using Question-Answer Pairs in Extractive Summarization of Email Conversations \cite{KATHLEEN07}
  \item Summarizing email conversations with clue words \cite{GIUSEPPE07}
\end{itemize}

\subparagraph{2009}
\begin{itemize}
  \item Regression-Based Summarization of Email Conversations \cite{JAN09}
\end{itemize}

\section{Conclusions}\label{conclusions}
    \begin{my_itemize}
        \item We will implement two classification algorithms and compare their results: SVM and Na\"{\i}ve Bayes.
        \item We will make use of the Enron dataset in learning and training phases.
        \item We will adopt an offline classification technique at the begining and will see if we can try an online technique later on.
        \item We will make use of all the features implemented in the above papers and try combining some of them to reach the best result.o
        \item We will use Weka (Waikato Environment for Knowledge Analysis) which is a collection of machine learning algorithms for data mining tasks , not to reimplement the classifications algrithms from the beginning 

    \end{my_itemize}

%=============================================================================================
% References
\begin{thebibliography}{99}
\bibitem{RON04}
  Ron Bekkerman,
  Andrew McCallum,
  Gary Huang,
  \emph{Automatic Categorization of Email into Folders: Benchmark Experiments on Enron and SRI Corpora},
  2004.

\bibitem{ANI03}
  Ani Nenkova,
  Amit Bagga,
  \emph{Email Classification for Contact Centers},
  2003.

\bibitem{JOSE11}
  Jose M. Carmona-Cejudo,
  Manuel Baena-Garcia,
  Jose del Campo-Avila,
  Rafael Morales-Bueno,
  Joao Gama,
  Albert Bifet,
  \emph{Using GNUsmail to Compare Data Stream Mining Methods for On-line Email Classification},
  2011.

\bibitem{NOUF08}
  Nouf Al Fe'ar,
  Einas Al Turki,
  Asma Al Zaid,
  Mashael Al Duwais,
  Mona Al Sheddi,
  Nora Al khamees,
  Nouf Al Drees,
  \emph{E-Classifier: A Bi-Lingual Email Classification System},
  2008.

\bibitem{NARESH10}
  Naresh Kumar Nagwani,
  Ashok Bhansali,
  \emph{An Object Oriented Email Clustering Model Using Weighted Similarities between Emails Attributes},
  2010.


\bibitem{BASKARAN09}
  S. Baskaran,
  \emph{Content Based Email Classification System by applying Conceptual Maps},
  2009.

\bibitem{CHAO08}
  Chao Zeng,
  Zhao Lu,
  Junzhong Gu,
  \emph{A new approach to Email classification using Concept Vector Space Model},
  2009.

\bibitem{BALAKUMAR08}
  M.Balakumar,
  V.Vaidehi,
  \emph{Ontology based classification and categorization of email},
  2008.

\bibitem{MIN11}
  Min-Feng Wang,
  Sie-Long Jheng,
  Meng-Feng Tsai,
  Cheng-Hsien Tang,
  \emph{Enterprise Email Classification Based on Social Network Features},
  2011.

\bibitem{MD07}
  Md Rafiqul Islam,
  Wanlei Zhou,
  \emph{Email Categorization Using Multi-Stage Classification Technique},
  2007.

\bibitem{YEHUDA11}
  Yehuda Koren,
  Edo Liberty,
  Yoelle Maarek,
  Roman Sandler,
  \emph{Automatically Tagging Email by Leveraging Other Users' Folders},
  2011.

\bibitem{WENQING05}
  Wenqing Zhao,
  Zili Zhang,
  \emph{An Email Classification Model Based on Rough Set Theory},
  2005.

\bibitem{sift01}
  Lokesh Shrestha,
  Kathleen McKeown,
  \emph{eMailSift: Email Classification Based on Structure and Content},
  2004.

\bibitem{sift02}
  Sharma Chakravarthy,
  Aravind Venkatachalam,
  Aditya Telang,
  \emph{ A Graph-Based Approach for Multi-Folder Email Classification},
  2010.

\bibitem{mous03}
  Taiwo Ayodele,
  Shikun Zhou,
  \emph{Applying Machine learning Algorithms for Email Management},
  2008.

\bibitem{mous04}
  Jason Chan,
  Irena Koprinska,
  Josiah Poon,
  \emph{Co-training with a Single Natural Feature Set Applied to Email Classification},
  2004.

\bibitem{mous05}
  Jason Chan,
  Irena Koprinska,
  Josiah Poon,
  \emph{Email Classification: Solution with Back Propagation Technique},
  2009.

\bibitem{LOKESH04}
  Lokesh Shrestha,
  Kathleen McKeown,
  \emph{Detection of question-answer pairs in email conversations},
  2004.

\bibitem{KATHLEEN07}
  Kathleen McKeown,
  Lokesh Shrestha,
  Owen Rambow,
  \emph{Using Question-Answer Pairs in Extractive Summarization of Email Conversations},
  2007.

\bibitem{GIUSEPPE07}
  Giuseppe Carenini,
  Raymond T. Ng,
  Xiaodong Zhou,
  \emph{Summarizing Email Conversations with Clue Words},
  2007.

\bibitem{SMAR01}
  Smaranda Muresan,
  Evelyne Tzoukermann,
  Judith L. Klavans,
  \emph{Combining Linguistic and Machine Learning Techniques for Email
Summarization},
  2001.

\bibitem{JAN09}
Jan Ulrich,
Giuseppe Carenini,
Gabriel Murray,
Raymond Ng
  \emph{Regression-Based Summarization of Email Conversations},
  2009.

\bibitem{SVETLANA01}
  Svetlana Kiritchenko,
  Stan Matwin,
  \emph{Email Classification with Co-Training},
  2001.

\bibitem{YUN08}
  Yun Fei Yi,
  Cheng Hua Li,
  Wei Song,
  \emph{Email classification Using Semantic Feature Space},
  2008.

\bibitem{MANUEL11}
  Jose M. Carmona-Cejudo,
  Manuel Baena-Garcia,
  Jose del Campo-Avila,
  Rafael Morales-Bueno,
  Albert Bifet,
  \emph{GNUsmail: Open Framework for On-line Email Classification},
  2011.
\end{thebibliography}
